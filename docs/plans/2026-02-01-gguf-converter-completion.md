# GGUF Converter Completion Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Complete the GGUF ‚Üí OD-MoE converter with integration tests and documentation

**Architecture:** The converter splits monolithic GGUF files into per-expert safetensors files + base model components for memory-mapped loading. Core extraction logic is complete; we need integration tests and documentation.

**Tech Stack:** Python, gguf library, safetensors, pytest, tqdm

---

## Current Status

‚úÖ **Already Implemented:**
- GGUF metadata parser with full tensor introspection
- Base model extraction (embeddings, attention, norms, LM head)
- Expert extraction to individual safetensors files (10,752 files for Kimi-K2.5)
- Toy model generator for testing (2 layers √ó 4 experts)
- Unit tests for parser, base extraction, expert extraction
- Progress bars with tqdm throughout extraction

üî≤ **Remaining Work:**
1. End-to-end integration test
2. Conversion documentation
3. Final verification

---

## Task 1: Write Integration Test

**Files:**
- Create: `tests/test_converter_integration.py`

**Step 1: Write the failing integration test**

Create a comprehensive end-to-end test that validates the full conversion pipeline.

```python
"""
Integration test for GGUF ‚Üí OD-MoE conversion.

Tests the complete pipeline:
1. Create toy GGUF model
2. Run full conversion
3. Verify all outputs exist and are valid
4. Verify expert registry is correct
5. Verify base model can be loaded
"""

import pytest
from pathlib import Path
import json
from convert.create_toy_model import create_toy_gguf_model
from convert.gguf_to_od_moe import convert_gguf_to_od_moe
from safetensors import safe_open


@pytest.fixture
def toy_gguf(tmp_path):
    """Create a toy GGUF file for testing"""
    gguf_path = tmp_path / "test.gguf"
    create_toy_gguf_model(
        output_path=gguf_path,
        num_layers=2,
        num_experts=4,
        dim=512,
        ffn_dim=1024
    )
    return gguf_path


def test_full_conversion_pipeline(toy_gguf, tmp_path):
    """Test complete GGUF ‚Üí OD-MoE conversion end-to-end."""
    output_dir = tmp_path / "od_moe_output"

    # Run full conversion
    convert_gguf_to_od_moe(
        input_path=str(toy_gguf),
        output_dir=str(output_dir),
        num_layers=2,
        num_experts=4
    )

    # Verify directory structure
    assert output_dir.exists()
    assert (output_dir / "base_model").exists()
    assert (output_dir / "experts").exists()

    # Verify base model files
    base_dir = output_dir / "base_model"
    assert (base_dir / "embeddings.safetensors").exists()
    assert (base_dir / "attention_layers.safetensors").exists()
    assert (base_dir / "norms.safetensors").exists()
    assert (base_dir / "lm_head.safetensors").exists()
    assert (base_dir / "metadata.json").exists()

    # Verify expert files (2 layers √ó 4 experts = 8 files)
    expert_dir = output_dir / "experts"
    safetensors_files = list(expert_dir.glob("*.safetensors"))
    assert len(safetensors_files) == 8

    # Verify registry
    registry_path = expert_dir / "registry.json"
    assert registry_path.exists()

    with open(registry_path) as f:
        registry = json.load(f)

    assert len(registry) == 8
    assert "layer_00_expert_000" in registry
    assert "layer_01_expert_003" in registry


def test_converted_base_model_loadable(toy_gguf, tmp_path):
    """Verify that converted base model safetensors files can be loaded."""
    output_dir = tmp_path / "od_moe_output"

    convert_gguf_to_od_moe(
        input_path=str(toy_gguf),
        output_dir=str(output_dir),
        num_layers=2,
        num_experts=4
    )

    # Try loading each base model file
    base_dir = output_dir / "base_model"

    with safe_open(base_dir / "embeddings.safetensors", framework="numpy") as f:
        assert "token_embd.weight" in f.keys()
        token_embd = f.get_tensor("token_embd.weight")
        assert token_embd.shape == (1000, 512)  # vocab_size=1000, dim=512

    with safe_open(base_dir / "attention_layers.safetensors", framework="numpy") as f:
        assert "blk.0.attn_q.weight" in f.keys()
        assert "blk.1.attn_q.weight" in f.keys()

    with safe_open(base_dir / "norms.safetensors", framework="numpy") as f:
        assert "blk.0.attn_norm.weight" in f.keys()
        assert "blk.0.ffn_norm.weight" in f.keys()


def test_converted_experts_loadable(toy_gguf, tmp_path):
    """Verify that all converted expert files can be loaded."""
    output_dir = tmp_path / "od_moe_output"

    convert_gguf_to_od_moe(
        input_path=str(toy_gguf),
        output_dir=str(output_dir),
        num_layers=2,
        num_experts=4
    )

    expert_dir = output_dir / "experts"

    # Try loading all expert files
    for layer in range(2):
        for expert in range(4):
            expert_file = expert_dir / f"layer_{layer:02d}_expert_{expert:03d}.safetensors"

            with safe_open(expert_file, framework="numpy") as f:
                # Verify all expert weights exist
                assert "w1.weight" in f.keys()
                assert "w2.weight" in f.keys()
                assert "w3.weight" in f.keys()

                # Verify shapes (dim=512, ffn_dim=1024)
                w1 = f.get_tensor("w1.weight")
                w2 = f.get_tensor("w2.weight")
                w3 = f.get_tensor("w3.weight")

                assert w1.shape == (512, 1024)
                assert w2.shape == (1024, 512)
                assert w3.shape == (512, 1024)


def test_expert_registry_completeness(toy_gguf, tmp_path):
    """Verify expert registry contains complete metadata for all experts."""
    output_dir = tmp_path / "od_moe_output"

    convert_gguf_to_od_moe(
        input_path=str(toy_gguf),
        output_dir=str(output_dir),
        num_layers=2,
        num_experts=4
    )

    registry_path = output_dir / "experts" / "registry.json"
    with open(registry_path) as f:
        registry = json.load(f)

    # Check each expert's metadata
    for layer in range(2):
        for expert in range(4):
            key = f"layer_{layer:02d}_expert_{expert:03d}"

            assert key in registry
            expert_info = registry[key]

            # Verify all required fields
            assert "path" in expert_info
            assert "size" in expert_info
            assert "layer" in expert_info
            assert "expert_id" in expert_info
            assert "tensors" in expert_info

            # Verify values
            assert expert_info["layer"] == layer
            assert expert_info["expert_id"] == expert
            assert expert_info["size"] > 0
            assert set(expert_info["tensors"]) == {"w1.weight", "w2.weight", "w3.weight"}
```

**Step 2: Run test to verify it passes**

Run: `pytest tests/test_converter_integration.py -v`
Expected: PASS (all conversion logic already implemented)

**Step 3: Commit**

```bash
git add tests/test_converter_integration.py
git commit -m "test: add end-to-end integration test for GGUF converter

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```

---

## Task 2: Write Conversion Documentation

**Files:**
- Create: `convert/README.md`

**Step 1: Write comprehensive conversion documentation**

Create user-facing documentation explaining the converter's purpose, usage, and outputs.

```markdown
# GGUF ‚Üí OD-MoE Converter

Converts monolithic GGUF models to per-expert safetensors files for memory-mapped on-demand loading.

## Purpose

Traditional MoE models store all experts in a single file, requiring the entire model to be loaded into memory. For massive models like Kimi-K2.5 (375GB), this requires 512GB+ RAM even with quantization.

This converter splits the model into:
- **Base model** (~35GB): Embeddings, attention layers, norms (always resident in memory)
- **Individual experts** (~30MB each): 10,752 separate files that can be memory-mapped and loaded on-demand

This enables running 375GB models on 192GB systems by keeping only a working set of experts in memory (~11GB) and streaming the rest from NVMe.

## Quick Start

### 1. Install Dependencies

```bash
pip install gguf safetensors tqdm numpy
```

### 2. Create a Toy Model for Testing

```bash
python -m convert.create_toy_model \
  --output convert/toy_model.gguf \
  --layers 2 \
  --experts 4 \
  --dim 512
```

This creates a ~2MB test model with the same structure as Kimi-K2.5.

### 3. Convert GGUF to OD-MoE Format

```bash
python -m convert.gguf_to_od_moe \
  --input convert/toy_model.gguf \
  --output /path/to/output \
  --num-layers 2 \
  --num-experts 4
```

For the full Kimi-K2.5 model:

```bash
python -m convert.gguf_to_od_moe \
  --input /Volumes/MacStudio/Kimi-K2.5.gguf \
  --output /Volumes/MacStudio/kimi_od_moe \
  --num-layers 28 \
  --num-experts 384
```

**‚ö†Ô∏è Warning:** Full conversion takes ~30-45 minutes and writes ~375GB to disk.

## Output Structure

```
output_dir/
‚îú‚îÄ‚îÄ base_model/
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.safetensors          # Token embeddings (vocab_size √ó dim)
‚îÇ   ‚îú‚îÄ‚îÄ attention_layers.safetensors    # All attention weights for 28 layers
‚îÇ   ‚îú‚îÄ‚îÄ norms.safetensors               # Layer norms
‚îÇ   ‚îú‚îÄ‚îÄ lm_head.safetensors             # Output projection
‚îÇ   ‚îî‚îÄ‚îÄ metadata.json                    # Component metadata
‚îÇ
‚îî‚îÄ‚îÄ experts/
    ‚îú‚îÄ‚îÄ layer_00_expert_000.safetensors  # Layer 0, Expert 0 (w1, w2, w3)
    ‚îú‚îÄ‚îÄ layer_00_expert_001.safetensors
    ‚îú‚îÄ‚îÄ ...
    ‚îú‚îÄ‚îÄ layer_27_expert_383.safetensors  # Layer 27, Expert 383
    ‚îî‚îÄ‚îÄ registry.json                     # Expert metadata (paths, sizes, indices)
```

### Base Model Components

**embeddings.safetensors** (~800MB for Kimi-K2.5)
- `token_embd.weight`: Token embeddings [vocab_size, dim]

**attention_layers.safetensors** (~30GB for Kimi-K2.5)
- `blk.{N}.attn_q.weight`: Query projection
- `blk.{N}.attn_k.weight`: Key projection
- `blk.{N}.attn_v.weight`: Value projection
- `blk.{N}.attn_output.weight`: Output projection
- `blk.{N}.ffn.gate.weight`: MoE router/gate

**norms.safetensors** (~4GB for Kimi-K2.5)
- `blk.{N}.attn_norm.weight`: Attention layer norm
- `blk.{N}.ffn_norm.weight`: FFN layer norm

**lm_head.safetensors** (~800MB for Kimi-K2.5)
- `output.weight`: Final language model head

### Expert Files

Each expert file contains the three weight matrices for a gated MLP:
- `w1.weight`: Gate projection [dim, ffn_dim]
- `w2.weight`: Down projection [ffn_dim, dim]
- `w3.weight`: Up projection [dim, ffn_dim]

For Kimi-K2.5:
- 28 layers √ó 384 experts = **10,752 expert files**
- Each file ~30MB
- Total: ~325GB

### Expert Registry

`experts/registry.json` contains metadata for all experts:

```json
{
  "layer_00_expert_000": {
    "path": "experts/layer_00_expert_000.safetensors",
    "size": 31457280,
    "layer": 0,
    "expert_id": 0,
    "tensors": ["w1.weight", "w2.weight", "w3.weight"]
  },
  ...
}
```

This registry enables fast expert lookup without scanning the filesystem.

## Memory-Mapped Loading

The converted files are designed for zero-copy loading using MLX:

```python
import mlx.core as mx
from safetensors import safe_open

# Load base model (always resident)
with safe_open("base_model/embeddings.safetensors", framework="mlx") as f:
    embeddings = f.get_tensor("token_embd.weight")  # Zero-copy

# Load expert on-demand
with safe_open("experts/layer_00_expert_042.safetensors", framework="mlx") as f:
    w1 = f.get_tensor("w1.weight")  # Zero-copy from mmap
```

## Performance

**Conversion (one-time cost):**
- Toy model (2 layers, 4 experts): ~1 second
- Kimi-K2.5 (28 layers, 384 experts): ~30-45 minutes

**Disk Space:**
- Input: 375GB GGUF file
- Output: 375GB (same size, different format)
- **Temporary space:** ~400GB peak (keep both during conversion)

**Expert Loading (runtime):**
- Cold expert (from NVMe): ~5ms
- Hot expert (LRU cache): ~0.1ms
- Working set: 8 experts √ó 28 layers √ó 30MB = ~6.7GB

## Validation

Run the test suite to verify conversion correctness:

```bash
# Unit tests
pytest tests/test_gguf_parser.py -v
pytest tests/test_base_extraction.py -v
pytest tests/test_expert_extraction.py -v

# Integration test
pytest tests/test_converter_integration.py -v
```

All tests should pass on the toy model.

## Troubleshooting

### "Not a GGUF file" Error

**Cause:** Input file is not a valid GGUF format.

**Solution:** Verify the file with `gguf-dump`:
```bash
python -c "import gguf; reader = gguf.GGUFReader('file.gguf'); print(reader.fields.keys())"
```

### Missing Expert Tensors

**Cause:** GGUF uses different tensor naming convention than expected.

**Solution:** Check actual tensor names:
```bash
python -c "import gguf; reader = gguf.GGUFReader('file.gguf'); print([t.name for t in reader.tensors if 'expert' in t.name][:10])"
```

Update `extract_experts()` to match your model's naming scheme.

### Out of Disk Space

**Cause:** Conversion requires ~2√ó model size temporarily.

**Solution:** Free up space or use `--output` on a larger drive:
```bash
python -m convert.gguf_to_od_moe \
  --input ~/models/kimi.gguf \
  --output /Volumes/ExternalSSD/kimi_od_moe
```

## Architecture Notes

This converter is part of the **mlx-od-moe** project, which implements on-demand MoE loading for Apple Silicon.

**Key insights:**
1. Apple Silicon has **unified memory** - no GPU/CPU split
2. The bottleneck is **working set size**, not data movement
3. NVMe on Mac Studio (~7GB/s) is fast enough to stream experts on-demand
4. Memory-mapped safetensors enable zero-copy loading from SSD

**Related components:**
- `mlx_od_moe/expert_store.py`: LRU cache + memory-mapped expert loading
- `mlx_od_moe/shadow_model.py`: Prefetch predictor (4 layers ahead)
- `mlx_od_moe/od_moe_layer.py`: On-demand expert loading during inference

See [ARCHITECTURE.md](../ARCHITECTURE.md) for the full system design.

## License

MIT
```

**Step 2: Verify documentation builds**

Run: `cat convert/README.md | head -50`
Expected: Documentation renders correctly

**Step 3: Commit**

```bash
git add convert/README.md
git commit -m "docs: add comprehensive GGUF converter documentation

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```

---

## Task 3: Final Verification

**Files:**
- Modify: `convert/gguf_to_od_moe.py` (verify progress bars work)

**Step 1: Run all converter tests**

Run the complete test suite to verify everything works:

```bash
pytest tests/test_gguf_parser.py \
       tests/test_base_extraction.py \
       tests/test_expert_extraction.py \
       tests/test_converter_integration.py \
       -v
```

Expected: All tests PASS

**Step 2: Test toy model conversion end-to-end**

```bash
# Create fresh toy model
python -m convert.create_toy_model --output /tmp/test_toy.gguf --layers 2 --experts 4

# Convert it
python -m convert.gguf_to_od_moe \
  --input /tmp/test_toy.gguf \
  --output /tmp/test_output \
  --num-layers 2 \
  --num-experts 4

# Verify output structure
ls -lh /tmp/test_output/base_model/
ls -lh /tmp/test_output/experts/ | head -20
cat /tmp/test_output/experts/registry.json | head -30
```

Expected:
- Conversion completes successfully with progress bars
- Base model files exist
- 8 expert files exist (2 layers √ó 4 experts)
- Registry JSON is valid

**Step 3: Verify progress bars display correctly**

The converter already has `tqdm` progress bars in:
- `extract_base_model()`: Reading tensors
- `extract_experts()`: Extracting experts

Run conversion again and verify progress bars show:
```
Reading tensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| XX/XX [00:00<00:00]
Extracting experts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00]
```

**Step 4: Final commit**

```bash
git add -A
git commit -m "feat: complete GGUF converter with integration tests and docs

- Add comprehensive end-to-end integration tests
- Add user-facing documentation with usage examples
- Verify all tests pass
- Verify progress bars work correctly

Converter is now production-ready for converting Kimi-K2.5 GGUF to OD-MoE format.

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```

---

## Testing Strategy

### Unit Tests (Already Passing)
- ‚úÖ `test_gguf_parser.py`: Metadata extraction, tensor listing
- ‚úÖ `test_base_extraction.py`: Base model component extraction
- ‚úÖ `test_expert_extraction.py`: Individual expert extraction

### Integration Tests (New)
- üÜï `test_converter_integration.py`: Full pipeline validation

### Manual Verification
- üÜï Toy model conversion with visual progress bars
- üÜï Documentation accuracy check

---

## Success Criteria

- ‚úÖ All unit tests pass
- ‚úÖ Integration test passes
- ‚úÖ Documentation is comprehensive and accurate
- ‚úÖ Progress bars display correctly during conversion
- ‚úÖ Toy model converts successfully in <5 seconds
- ‚úÖ Output structure matches specification

---

## Execution Handoff

Plan complete and saved to `docs/plans/2026-02-01-gguf-converter-completion.md`. Two execution options:

**1. Subagent-Driven (this session)** - I dispatch fresh subagent per task, review between tasks, fast iteration

**2. Parallel Session (separate)** - Open new session with executing-plans, batch execution with checkpoints

Which approach?
